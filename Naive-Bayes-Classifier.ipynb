{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import sympy\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<h1 style=\"color: blue;\">Overview of the basics</h1>\n",
    "\n",
    "For a random variable $x$, $P(x)$ is a function that assigns a probability to all values of $x$.\n",
    "\n",
    "- Probability Density of $x = P(x)$\n",
    "\n",
    "The probability of a specific event A for a random variable x is denoted as P(x=A), or simply as P(A).\n",
    "\n",
    "- Probability of Event $A = P(A)$\n",
    "\n",
    "Probability is calculated as the number of desired outcomes divided by the total possible outcomes, in the case where all outcomes are equally likely.\n",
    "\n",
    "- Probability = ${\\large \\frac{number-of-desired-outcomes}{total-number-of-possible-outcomes}}$\n",
    "\n",
    ">This is intuitive if we think about a discrete random variable such as the roll of a dice. For example, the probability of a dice rolling a 5 is calculated as one outcome of rolling a 5 (1) divided by the total number of discrete outcomes (6) or 1/6 or about 0.1666 or about 16.666%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of taking 2 red ball(s) is: 8.7%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08695652173913043"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is a simple preview of probability calculator\n",
    "def probability(desired_outcomes, total_outcomes):\n",
    "    probability = desired_outcomes / total_outcomes\n",
    "    \n",
    "    if desired_outcomes == 12:\n",
    "        color = \"green\"\n",
    "    elif desired_outcomes == 8:\n",
    "        color = \"blue\"\n",
    "    elif desired_outcomes == 2:\n",
    "        color = \"red\"\n",
    "    elif desired_outcomes == total_outcomes:\n",
    "        color = \"(all)\"\n",
    "    else:\n",
    "        color = \"yellow\"\n",
    "  \n",
    "    print(f\"The probability of taking {desired_outcomes} {color} ball(s) is:\", str(round(probability*100, 2))+\"%\")\n",
    "    return probability # Probability is a number between 0 and 1. 1 Denoting 100% and 0 denoting 0%\n",
    "\n",
    "# Let's say we have 12 Green balls, 8 blue balls, 2 red balls and 1 yellow ball\n",
    "# What is the probability of reaching and taking 1 yellow ball?\n",
    "\n",
    "total_balls = 12 + 8 + 2 + 1\n",
    "desired_balls = 2 # Play around with this variable to see how different portions react\n",
    "\n",
    "probability(desired_balls, total_balls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of the probabilities of all outcomes must equal one. If not, we do not have valid probabilities.\n",
    "\n",
    "- Sum of the Probabilities for All Outcomes = 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of taking 12 green ball(s) is: 52.17%\n",
      "The probability of taking 8 blue ball(s) is: 34.78%\n",
      "The probability of taking 2 red ball(s) is: 8.7%\n",
      "The probability of taking 1 yellow ball(s) is: 4.35%\n",
      "Sum of all probabilities: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "green_balls = 12\n",
    "blue_balls = 8\n",
    "red_balls = 2\n",
    "yellow_balls = 1\n",
    "\n",
    "total_sum = probability(green_balls, total_balls) + probability(blue_balls,total_balls) + probability(red_balls,total_balls)+ probability(yellow_balls,total_balls) \n",
    "\n",
    "print(\"Sum of all probabilities:\", total_sum*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of an impossible outcome is zero. For example, it is impossible to roll a 7 with a standard six-sided die.\n",
    "\n",
    "- Probability of Impossible Outcome = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of a certain outcome is one. For example, it is certain that a value between 1 and 6 will occur when rolling a six-sided die.\n",
    "\n",
    "- Probability of Certain Outcome = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of taking 23 (all) ball(s) is: 100.0%\n",
      "Probability of certain outcome: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Probability of certain outcome:\", probability(total_balls, total_balls)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of an event not occurring, called the complement.\n",
    "\n",
    "This can be calculated by one minus the probability of the event, or 1 – P(A). For example, the probability of not rolling a 5 would be 1 – P(5) or 1 – 0.166 or about 0.833 or about 83.333%.\n",
    "\n",
    "- Probability of Not Event $A = 1 – P(A)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of taking 12 green ball(s) is: 52.17%\n",
      "Probability of Not Event: 47.83 %\n"
     ]
    }
   ],
   "source": [
    "# Lets take the probability of 12 green balls\n",
    "green_balls = 12\n",
    "draw_probability = probability(green_balls, total_balls)\n",
    "\n",
    "print(\"Probability of Not Event:\", str(round((1 - draw_probability)*100, 2)), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem of Conditional Probability\n",
    "Before diving into Bayes theorem, let’s review marginal, joint, and conditional probability.\n",
    "\n",
    "Recall that marginal probability is the probability of an event, irrespective of other random variables. If the random variable is independent, then it is the probability of the event directly, otherwise, if the variable is dependent upon other variables, then the marginal probability is the probability of the event summed over all outcomes for the dependent variables, called the sum rule.\n",
    "\n",
    "- **Marginal Probability**: The probability of an event irrespective of the outcomes of other random variables, e.g. $P(A)$.\n",
    "\n",
    "The joint probability is the probability of two (or more) simultaneous events, often described in terms of events A and B from two dependent random variables, e.g. X and Y. The joint probability is often summarized as just the outcomes, e.g. A and B.\n",
    "\n",
    "- **Joint Probability**: Probability of two (or more) simultaneous events, e.g. $P(A and B)$ or $P(A, B)$.\n",
    "\n",
    "The conditional probability is the probability of one event given the occurrence of another event, often described in terms of events A and B from two dependent random variables e.g. X and Y.\n",
    "\n",
    "- **Conditional Probability**: Probability of one (or more) event given the occurrence of another event, e.g. $P(A given B)$ or $P(A | B)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint probability distribution\n",
    "Now let's see what happens if we roll two dice. For each die, the outcomes are associated with a certain probability. We need two random variables to describe the game, let's say that $\\text{x}$ corresponds to the first die and $\\text{y}$ to the second one. We also have two probability mass functions associated with the random variables: $P(\\text{x})$ and $P(\\text{y})$. Here the possible values of the random variables (1, 2, 3, 4, 5 or 6) and the probability mass functions are actually the same for both dice, but it doesn't need to be the case.\n",
    "\n",
    "The joint probability distribution is useful in the cases where we are interested in the probability that $\\text{x}$ takes a specific value while $\\text{y}$ takes another specific value. For instance, what would be the probability to get a 1 with the first dice and 2 with the second dice? The probabilities corresponding to every pair of values are written $P(\\text{x}=x, \\text{y}=y)$ or $P(\\text{x}, \\text{y})$. This is what we call the joint probability.\n",
    "\n",
    "### Example 1.\n",
    "\n",
    "For example, let's calculate the probability to have a 1 with the first dice and a 2 in the second:\n",
    "\n",
    "$$\n",
    "P(\\text{x}=1, \\text{y}=2) = \\frac{1}{6} \\times \\frac{1}{6} = \\frac{1}{36} \\approx 0.028\n",
    "$$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional probability is the probability of one event given the occurrence of another event, often described in terms of events A and B from two dependent random variables e.g. $X$ and $Y$.\n",
    "\n",
    "- Conditional Probability: Probability of one (or more) event given the occurrence of another event, e.g. $P(A given B)$ or $P(A | B)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint probability can be calculated using the conditional probability; for example:\n",
    "\n",
    "- $P(A, B) = P(A | B) * P(B)$\n",
    "\n",
    "This is called the product rule. Importantly, the joint probability is symmetrical, meaning that:\n",
    "\n",
    "- $P(A, B) = P(B, A)$\n",
    "\n",
    "The conditional probability can be calculated using the joint probability; for example:\n",
    "\n",
    "- $P(A | B) = P(A, B) / P(B)$\n",
    "\n",
    "The conditional probability is not symmetrical; for example:\n",
    "\n",
    "- $P(A | B) != P(B | A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Naive Bayes algorithm?\n",
    "\n",
    "   It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "\n",
    "   For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.\n",
    "\n",
    "   Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n",
    "\n",
    "   Bayes theorem provides a way of calculating posterior probability $P(c|x)$ from $P(c)$, $P(x)$ and $P(x|c)$. Look at the equation below:\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{align}\n",
    "{\\large P(c|x) = \\frac{P(x|c)P(c)}{P(x)}}\n",
    "\\end{align}\n",
    "\n",
    "<br>\n",
    "\n",
    "## Naming the Terms in the Theorem\n",
    "The terms in the Bayes Theorem equation are given names depending on the context where the equation is used.\n",
    "\n",
    "Firstly, in general, the result $P(A|B)$ is referred to as the **posterior probability** and $P(A$) is referred to as the **prior probability**.\n",
    "\n",
    "- $P(A|B)$: Posterior probability.\n",
    "- $P(A)$: Prior probability.\n",
    "\n",
    "Sometimes $P(B|A)$ is referred to as the **likelihood** and $P(B)$ is referred to as the **evidence**.\n",
    "\n",
    "- $P(B|A)$: Likelihood.\n",
    "- $P(B)$: Evidence.\n",
    "\n",
    "This allows Bayes Theorem to be restated as:\n",
    "\n",
    "- **Posterior = Likelihood * Prior / Evidence**\n",
    "\n",
    "We can make this clear with a smoke and fire case.\n",
    "\n",
    "What is the probability that there is fire given that there is smoke?\n",
    "\n",
    "Where P(Fire) is the Prior, P(Smoke|Fire) is the Likelihood, and P(Smoke) is the evidence:\n",
    "\n",
    "- $P(Fire|Smoke)$ = $P(Smoke|Fire)$ * $P(Fire) / P(Smoke)$\n",
    "\n",
    "You can imagine the same situation with rain and clouds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostic Test Scenario\n",
    "An excellent and widely used example of the benefit of Bayes Theorem is in the analysis of a medical diagnostic test.\n",
    "\n",
    "Scenario: Consider a human population that may or may not have cancer (Cancer is True or False) and a medical test that returns positive or negative for detecting cancer (Test is Positive or Negative), e.g. like a mammogram for detecting breast cancer.\n",
    "\n",
    ">Problem: If a randomly selected patient has the test and it comes back positive, what is the probability that the patient has cancer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Calculation\n",
    "Medical diagnostic tests are not perfect; they have **error**.\n",
    "\n",
    "Sometimes a patient will have cancer, but the test will not detect it. This capability of the test to detect cancer is referred to as the sensitivity, or the **true positive rate**.\n",
    "\n",
    "In this case, we will contrive a sensitivity value for the test. The test is good, but not great, with a true positive rate or sensitivity of $85%$. That is, of all the people who have cancer and are tested, $85%$ of them will get a positive result from the test.\n",
    "\n",
    "- $P(Test=Positive | Cancer=True) = 0.85$\n",
    "\n",
    "Given this information, our intuition would suggest that there is an 85% probability that the patient has cancer.\n",
    "\n",
    "**Our intuitions of probability are wrong.**\n",
    "\n",
    "This type of error in interpreting probabilities is so common that it has its own name; it is referred to as the **base rate fallacy**.\n",
    "\n",
    "It has this name because the error in estimating the probability of an event is caused by ignoring the base rate. That is, it ignores the probability of a randomly selected person having cancer, regardless of the results of a diagnostic test.\n",
    "\n",
    "In this case, we can assume the probability of breast cancer is low, and use a contrived base rate value of one person in 5,000, or (0.0002) 0.02%.\n",
    "\n",
    "- $P(Cancer=True) = 0.02%$.\n",
    "\n",
    "We can correctly calculate the probability of a patient having cancer given a positive test result using Bayes Theorem.\n",
    "\n",
    "Let’s map our scenario onto the equation:\n",
    "\n",
    "- $P(A|B) = P(B|A) * P(A) / P(B)$\n",
    "\n",
    "- $P(Cancer=True | Test=Positive)$ = $P(Test=Positive|Cancer=True)$ * $P(Cancer=True)$ / $P(Test=Positive)$\n",
    "\n",
    "We know the probability of the test being positive given that the patient has cancer is 85%, and we know the base rate or the prior probability of a given patient having cancer is 0.02%; we can plug these values in:\n",
    "\n",
    "- $P(Cancer=True | Test=Positive)$ = 0.85 * 0.0002 / $P(Test=Positive)$\n",
    "\n",
    "We **don’t know** $P(Test=Positive)$, it’s not given directly.\n",
    "\n",
    "Instead, we can estimate it using:\n",
    "\n",
    "- $P(B)$ = $P(B|A)$ * $P(A)$ + $P(B|not A)$ * $P(not A)$\n",
    "\n",
    "- $P(Test=Positive)$ = $P(Test=Positive|Cancer=True)$ * $P(Cancer=True)$ + $P(Test=Positive|Cancer=False)$ * $P(Cancer=False)$\n",
    "\n",
    "Firstly, we can calculate $P(Cancer=False)$ as the complement of $P(Cancer=True)$, which we already know\n",
    "\n",
    "- $P(Cancer=False)$ = 1 – $P(Cancer=True)$\n",
    "- = 1 – 0.0002\n",
    "- = 0.9998\n",
    "\n",
    "Let’s plugin what we have:\n",
    "\n",
    "We can plug in our known values as follows:\n",
    "\n",
    "- $P(Test=Positive)$ = 0.85 * 0.0002 + $P(Test=Positive|Cancer=False)$ * 0.9998\n",
    "\n",
    "We still do not know the probability of a positive test result given no cancer.\n",
    "\n",
    "This requires additional information.\n",
    "\n",
    "Specifically, we need to know how good the test is at correctly identifying people that do not have cancer. That is, testing negative result $(Test=Negative)$ when the patient does not have cancer $(Cancer=False)$, called the **true negative rate** or the specificity.\n",
    "\n",
    "We will use a contrived specificity value of 95%.\n",
    "\n",
    "- $P(Test=Negative | Cancer=False)$ = 0.95\n",
    "\n",
    "With this final piece of information, we can calculate the **false positive** or **false alarm rate** as the complement of the **true negative rate**.\n",
    "\n",
    "- $P(Test=Positive|Cancer=False)$ = 1 – $P(Test=Negative | Cancer=False)$\n",
    "- = 1 – 0.95\n",
    "- = 0.05\n",
    "\n",
    "We can plug this false alarm rate into our calculation of $P(Test=Positive)$ as follows:\n",
    "\n",
    "- $P(Test=Positive)$ = 0.85 * 0.0002 + 0.05 * 0.9998\n",
    "- $P(Test=Positive)$ = 0.00017 + 0.04999\n",
    "- $P(Test=Positive)$ = 0.05016\n",
    "\n",
    "Excellent, so the probability of the test returning a positive result, regardless of whether the person has cancer or not is about **5%**.\n",
    "\n",
    "We now have enough information to calculate Bayes Theorem and estimate the probability of a randomly selected person having cancer if they get a positive test result.\n",
    "\n",
    "- $P(Cancer=True | Test=Positive)$ = $P(Test=Positive|Cancer=True)$ * $P(Cancer=True)$ / $P(Test=Positive)$\n",
    "- $P(Cancer=True | Test=Positive)$ = 0.85 * 0.0002 / 0.05016\n",
    "- $P(Cancer=True | Test=Positive)$ = 0.00017 / 0.05016\n",
    "- $P(Cancer=True | Test=Positive)$ = 0.003389154704944\n",
    "\n",
    "The calculation suggests that if the patient is informed they have cancer with this test, then there is only 0.33% chance that they have cancer.\n",
    "\n",
    "**It is a terrible diagnostic test!**\n",
    "\n",
    "The example also shows that the calculation of the conditional probability requires enough information.\n",
    "\n",
    "For example, if we have the values used in Bayes Theorem already, we can use them directly.\n",
    "\n",
    "This is rarely the case, and we typically have to calculate the bits we need and plug them in, as we did in this case. In our scenario we were given 3 pieces of information, the base rate, the  sensitivity (or true positive rate), and the specificity (or true negative rate).\n",
    "\n",
    "- Sensitivity: 85% of people with cancer will get a positive test result.\n",
    "- Base Rate: 0.02% of people have cancer.\n",
    "- Specificity: 95% of people without cancer will get a negative test result.\n",
    "\n",
    "I did not have the $P(Test=Positive)$, but we calculated it given what we already had available.\n",
    "\n",
    "We might imagine that Bayes Theorem allows us to be even more precise about a given scenario. For example, if we had more information about the patient (e.g. their age) and about the domain (e.g. cancer rates for age ranges), and in turn we could offer an even more accurate probability estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(A|B) = 0.339%\n"
     ]
    }
   ],
   "source": [
    "# calculate the probability of cancer patient and diagnostic test\n",
    " \n",
    "# calculate P(A|B) given P(A), P(B|A), P(B|not A)\n",
    "def bayes_theorem(p_a, p_b_given_a, p_b_given_not_a):\n",
    "    # calculate P(not A)\n",
    "    not_a = 1 - p_a\n",
    "    # calculate P(B)\n",
    "    p_b = p_b_given_a * p_a + p_b_given_not_a * not_a\n",
    "    # calculate P(A|B)\n",
    "    p_a_given_b = (p_b_given_a * p_a) / p_b\n",
    "    return p_a_given_b\n",
    " \n",
    "# P(A)\n",
    "p_a = 0.0002\n",
    "# P(B|A)\n",
    "p_b_given_a = 0.85\n",
    "# P(B|not A)\n",
    "p_b_given_not_a = 0.05\n",
    "# calculate P(A|B)\n",
    "result = bayes_theorem(p_a, p_b_given_a, p_b_given_not_a)\n",
    "# summarize\n",
    "print('P(A|B) = %.3f%%' % (result * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classifier Terminology\n",
    "It may be helpful to think about the cancer test example in terms of the common terms from binary (two-class) classification, i.e. where notions of specificity and sensitivity come from.\n",
    "\n",
    "Personally, I find these terms help everything to make sense.\n",
    "\n",
    "Firstly, let’s define a confusion matrix:\n",
    "\n",
    "                    | Positive Class      | Negative Class\n",
    "Positive Prediction | True Positive (TP)  | False Positive (FP)\n",
    "Negative Prediction | False Negative (FN) | True Negative (TN)\n",
    "1\n",
    "2\n",
    "3\n",
    "                    | Positive Class      | Negative Class\n",
    "Positive Prediction | True Positive (TP)  | False Positive (FP)\n",
    "Negative Prediction | False Negative (FN) | True Negative (TN)\n",
    "We can then define some rates from the confusion matrix:\n",
    "\n",
    "True Positive Rate (TPR) = TP / (TP + FN)\n",
    "False Positive Rate (FPR) = FP / (FP + TN)\n",
    "True Negative Rate (TNR) = TN / (TN + FP)\n",
    "False Negative Rate (FNR) = FN / (FN + TP)\n",
    "These terms are called rates, but they can also be interpreted as probabilities.\n",
    "\n",
    "Also, it might help to notice:\n",
    "\n",
    "TPR + FNR = 1.0, or:\n",
    "FNR = 1.0 – TPR\n",
    "TPR = 1.0 – FNR\n",
    "TNR + FPR = 1.0, or:\n",
    "TNR = 1.0 – FPR\n",
    "FPR = 1.0 – TNR\n",
    "Recall that in a previous section that we calculated the false positive rate given the complement of true negative rate, or FPR = 1.0 – TNR.\n",
    "\n",
    "Some of these rates have special names, for example:\n",
    "\n",
    "Sensitivity = TPR\n",
    "Specificity = TNR\n",
    "We can map these rates onto familiar terms from Bayes Theorem:\n",
    "\n",
    "P(B|A): True Positive Rate (TPR).\n",
    "P(not B|not A): True Negative Rate (TNR).\n",
    "P(B|not A): False Positive Rate (FPR).\n",
    "P(not B|A): False Negative Rate (FNR).\n",
    "We can also map the base rates for the condition (class) and the treatment (prediction) on familiar terms from Bayes Theorem:\n",
    "\n",
    "P(A): Probability of a Positive Class (PC).\n",
    "P(not A): Probability of a Negative Class (NC).\n",
    "P(B): Probability of a Positive Prediction (PP).\n",
    "P(not B): Probability of a Negative Prediction (NP).\n",
    "Now, let’s consider Bayes Theorem using these terms:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "P(A|B) = (TPR * PC) / PP\n",
    "Where we often cannot calculate P(B), so we use an alternative:\n",
    "\n",
    "P(B) = P(B|A) * P(A) + P(B|not A) * P(not A)\n",
    "P(B) = TPR * PC + FPR * NC\n",
    "Now, let’s look at our scenario of cancer and a cancer detection test.\n",
    "\n",
    "The class or condition would be “Cancer” and the treatment or prediction would the “Test“.\n",
    "\n",
    "First, let’s review all of the rates:\n",
    "\n",
    "True Positive Rate (TPR): 85%\n",
    "False Positive Rate (FPR): 5%\n",
    "True Negative Rate (TNR): 95%\n",
    "False Negative Rate (FNR): 15%\n",
    "Let’s also review what we know about base rates:\n",
    "\n",
    "Positive Class (PC): 0.02%\n",
    "Negative Class (NC): 99.98%\n",
    "Positive Prediction (PP): 5.016%\n",
    "Negative Prediction (NP): 94.984%\n",
    "Plugging things in, we can calculate the probability of a positive test result (a positive prediction) as the probability of a positive test result given cancer (the true positive rate) multiplied by the base rate for having cancer (the positive class), plus the probability if a positive test result given no cancer (the false positive rate) plus the probability of not having cancer (the negative class).\n",
    "\n",
    "The calculation with these terms is as follows:\n",
    "\n",
    "P(B) = P(B|A) * P(A) + P(B|not A) * P(not A)\n",
    "P(B) = TPR * PC + FPR * NC\n",
    "P(B) = 85% * 0.02% + 5% * 99.98%\n",
    "P(B) = 5.016%\n",
    "We can then calculate Bayes Theorem for the scenario, namely the probability of cancer given a positive test result (the posterior) is the probability of a positive test result given cancer (the true positive rate) multiplied by the probability of having cancer (the positive class rate), divided by the probability of a positive test result (a positive prediction).\n",
    "\n",
    "The calculation with these terms is as follows:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "P(A|B) = TPR * PC / PP\n",
    "P(A|B) = 85% * 0.02% / 5.016%\n",
    "P(A|B) = 0.339%\n",
    "It turns out that in this case, the posterior probability that we are calculating with the Bayes theorem is equivalent to the precision, also called the Positive Predictive Value (PPV) of the confusion matrix:\n",
    "\n",
    "PPV = TP / (TP + FP)\n",
    "Or, stated in our classifier terms:\n",
    "\n",
    "P(A|B) = PPV\n",
    "PPV = TPR * PC / PP\n",
    "So why do we go to all of the trouble of calculating the posterior probability?\n",
    "\n",
    "Because we don’t have the confusion matrix for a population of people both with and without cancer that have been tested and have been not tested. Instead, all we have is some priors and probabilities about our population and our test.\n",
    "\n",
    "This highlights when we might choose to use the calculation in practice.\n",
    "\n",
    "Specifically, when we have beliefs about the events involved, but we cannot perform the calculation by counting examples in the real world.                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Naive Bayes algorithm work?\n",
    "Let’s understand it using an example. Below I have taken a training data set of weather and corresponding target variable ‘Play’ (suggesting possibilities of playing). Now, we need to classify whether players will play or not based on weather condition.\n",
    "\n",
    "- Step 1: Convert the data set into a frequency table\n",
    "\n",
    "- Step 2: Create Likelihood table by finding the probabilities like Overcast probability = 0.29 and probability of playing is 0.64.\n",
    "\n",
    "<img src=\"./images/NaiveBayesianFootball.webp\"/>\n",
    "\n",
    "- Step 3: Now, use Naive Bayesian equation to calculate the posterior probability for each class. The class with the highest posterior probability is the outcome of prediction.\n",
    "\n",
    "**Problem: Players will play if weather is sunny. Is this statement correct?**\n",
    "\n",
    "Now let's solve it.\n",
    "\n",
    "$P(Yes | Sunny)$ = $P( Sunny | Yes)$ * $P(Yes)$ / $P(Sunny)$\n",
    "\n",
    "Here we have $P(Sunny |Yes)$ = 3/9 = 0.33, $P(Sunny)$ = 5/14 = 0.36, $P(Yes)$= 9/14 = 0.64\n",
    "\n",
    "Now, $P(Yes | Sunny)$ = 0.33 * 0.64 / 0.36 = 0.60, which has higher probability.\n",
    "\n",
    "Naive Bayes uses a similar method to predict the probability of different class based on various attributes. This algorithm is mostly used in text classification and with problems having multiple classes.\n",
    "\n",
    "## Some of the applications of Naive Bayes Algorithms\n",
    "Real time Prediction: Naive Bayes is an eager learning classifier and it is sure fast. Thus, it could be used for making predictions in real time.\n",
    "- Multi class Prediction: This algorithm is also well known for multi class prediction feature. Here we can predict the probability of multiple classes of target variable.\n",
    "- Text classification/ Spam Filtering/ Sentiment Analysis: Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments)\n",
    "- Recommendation System: Naive Bayes Classifier and Collaborative Filtering together builds a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Flower Species Dataset\n",
    "In this tutorial we will use the Iris Flower Species Dataset.\n",
    "\n",
    "The Iris Flower Dataset involves predicting the flower species given measurements of iris flowers.\n",
    "\n",
    "It is a multiclass classification problem. The number of observations for each class is balanced. There are 150 observations with 4 input variables and 1 output variable. The variable names are as follows:\n",
    "\n",
    "- Sepal length in cm.\n",
    "- Sepal width in cm.\n",
    "- Petal length in cm.\n",
    "- Petal width in cm.\n",
    "- Class\n",
    "\n",
    "A sample of the first 4 rows is listed below.\n",
    "```python\n",
    "4.9,3.0,1.4,0.2,Iris-setosa\n",
    "4.7,3.2,1.3,0.2,Iris-setosa\n",
    "4.6,3.1,1.5,0.2,Iris-setosa\n",
    "5.0,3.6,1.4,0.2,Iris-setosa\n",
    "...\n",
    "```\n",
    "\n",
    "<a href=\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.names\">More Information on Dataset (iris.names)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class\n",
       "0           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "1           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "2           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "3           5.0          3.6           1.4          0.2  Iris-setosa\n",
       "4           5.4          3.9           1.7          0.4  Iris-setosa"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Url from which we will download our iris.csv dataset\n",
    "iris = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(iris)\n",
    "\n",
    "# Read the file and asign the data in 'df'\n",
    "df = pd.read_csv(iris, sep=',')\n",
    "\n",
    "# Add column names to the DataFrame\n",
    "attributes = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n",
    "df.columns = attributes\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: orange;\">Step 1: Separate By Class</h1>\n",
    "\n",
    "We will need to calculate the probability of data by the class they belong to, the so-called base rate.\n",
    "\n",
    "This means that we will first need to separate our training data by class.\n",
    "\n",
    "We can create a dictionary object where each key is the class value and then add a list of all the records as the value in the dictionary.\n",
    "\n",
    "Below is a function named **separate_by_class()** that implements this approach. It assumes that the last column in each row is the class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[3.393533211, 2.331273381, 0]\n",
      "[3.110073483, 1.781539638, 0]\n",
      "[1.343808831, 3.368360954, 0]\n",
      "[3.582294042, 4.67917911, 0]\n",
      "[2.280362439, 2.866990263, 0]\n",
      "1\n",
      "[7.423436942, 4.696522875, 1]\n",
      "[5.745051997, 3.533989803, 1]\n",
      "[9.172168622, 2.511101045, 1]\n",
      "[7.792783481, 3.424088941, 1]\n",
      "[7.939820817, 0.791637231, 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class\n",
       "0           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "1           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "2           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "3           5.0          3.6           1.4          0.2  Iris-setosa\n",
       "4           5.4          3.9           1.7          0.4  Iris-setosa"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset by class values, returns a dictionary\n",
    "def separate_by_class(dataset):\n",
    "    separated = dict()\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        class_value = vector[-1]\n",
    "        if (class_value not in separated):\n",
    "            separated[class_value] = list()\n",
    "        separated[class_value].append(vector)\n",
    "    return separated\n",
    "\n",
    "# Test separating data by class\n",
    "dataset = [[3.393533211,2.331273381,0],\n",
    "    [3.110073483,1.781539638,0],\n",
    "    [1.343808831,3.368360954,0],\n",
    "    [3.582294042,4.67917911,0],\n",
    "    [2.280362439,2.866990263,0],\n",
    "    [7.423436942,4.696522875,1],\n",
    "    [5.745051997,3.533989803,1],\n",
    "    [9.172168622,2.511101045,1],\n",
    "    [7.792783481,3.424088941,1],\n",
    "    [7.939820817,0.791637231,1]]\n",
    "\n",
    "separated = separate_by_class(dataset)\n",
    "\n",
    "for label in separated:\n",
    "    print(label)\n",
    "    for row in separated[label]:\n",
    "        print(row)\n",
    "        \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: orange;\">Step 2: Summarize Dataset</h1>\n",
    "\n",
    "We need two statistics from a given set of data.\n",
    "\n",
    "We’ll see how these statistics are used in the calculation of probabilities in a few steps. The two statistics we require from a given dataset are the mean and the standard deviation (average deviation from the mean).\n",
    "\n",
    "The mean is the average value and can be calculated as:\n",
    "\n",
    "- mean = sum(x)/n * count(x)\n",
    "\n",
    "Where x is the list of values or a column we are looking.\n",
    "\n",
    "Below is a small function named mean() that calculates the mean of a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of a list of numbers\n",
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: #FF4500;\">In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range.</p>\n",
    "\n",
    "The sample standard deviation is calculated as the mean difference from the mean value. This can be calculated as:\n",
    "\n",
    "- standard deviation = sqrt((sum i to N (x_i – mean(x))^2) / N-1)\n",
    "\n",
    "You can see that we square the difference between the mean and a given value, calculate the average squared difference from the mean, then take the square root to return the units back to their original value.\n",
    "\n",
    "Below is a small function named standard_deviation() that calculates the standard deviation of a list of numbers. You will notice that it calculates the mean. It might be more efficient to calculate the mean of a list of numbers once and pass it to the standard_deviation() function as a parameter. You can explore this optimization if you’re interested later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of a list of numbers\n",
    "def stdev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
    "    return sqrt(variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We require the mean and standard deviation statistics to be calculated for each input attribute or each column of our data.\n",
    "\n",
    "We can do that by gathering all of the values for each column into a list and calculating the mean and standard deviation on that list. Once calculated, we can gather the statistics together into a list or tuple of statistics. Then, repeat this operation for each column in the dataset and return a list of tuples of statistics.\n",
    "\n",
    "Below is a function named **summarize_dataset()** that implements this approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean, stdev and count for each column in a dataset\n",
    "def summarize_dataset(dataset):\n",
    "    summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
    "    del(summaries[-1])\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
