{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "\n",
    "from math import sqrt\n",
    "from math import pi\n",
    "from math import exp\n",
    "\n",
    "import sympy\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<h1 style=\"color: blue;\">Overview of the basics</h1>\n",
    "\n",
    "For a random variable $x$, $P(x)$ is a function that assigns a probability to all values of $x$.\n",
    "\n",
    "- Probability Density of $x = P(x)$\n",
    "\n",
    "The probability of a specific event A for a random variable x is denoted as P(x=A), or simply as P(A).\n",
    "\n",
    "- Probability of Event $A = P(A)$\n",
    "\n",
    "Probability is calculated as the number of desired outcomes divided by the total possible outcomes, in the case where all outcomes are equally likely.\n",
    "\n",
    "- Probability = ${\\large \\frac{number-of-desired-outcomes}{total-number-of-possible-outcomes}}$\n",
    "\n",
    ">This is intuitive if we think about a discrete random variable such as the roll of a dice. For example, the probability of a dice rolling a 5 is calculated as one outcome of rolling a 5 (1) divided by the total number of discrete outcomes (6) or 1/6 or about 0.1666 or about 16.666%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of taking 2 red ball(s) is: 8.7%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08695652173913043"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is a simple preview of probability calculator\n",
    "def probability(desired_outcomes, total_outcomes):\n",
    "    probability = desired_outcomes / total_outcomes\n",
    "    \n",
    "    if desired_outcomes == 12:\n",
    "        color = \"green\"\n",
    "    elif desired_outcomes == 8:\n",
    "        color = \"blue\"\n",
    "    elif desired_outcomes == 2:\n",
    "        color = \"red\"\n",
    "    elif desired_outcomes == total_outcomes:\n",
    "        color = \"(all)\"\n",
    "    else:\n",
    "        color = \"yellow\"\n",
    "  \n",
    "    print(f\"The probability of taking {desired_outcomes} {color} ball(s) is:\", str(round(probability*100, 2))+\"%\")\n",
    "    return probability # Probability is a number between 0 and 1. 1 Denoting 100% and 0 denoting 0%\n",
    "\n",
    "# Let's say I have 12 Green balls, 8 blue balls, 2 red balls and 1 yellow ball\n",
    "# What is the probability of reaching and taking 1 yellow ball?\n",
    "\n",
    "total_balls = 12 + 8 + 2 + 1\n",
    "desired_balls = 2 # Play around with this variable to see how different portions react\n",
    "\n",
    "probability(desired_balls, total_balls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of the probabilities of all outcomes must equal one. If not, I do not have valid probabilities.\n",
    "\n",
    "- Sum of the Probabilities for All Outcomes = 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of taking 12 green ball(s) is: 52.17%\n",
      "The probability of taking 8 blue ball(s) is: 34.78%\n",
      "The probability of taking 2 red ball(s) is: 8.7%\n",
      "The probability of taking 1 yellow ball(s) is: 4.35%\n",
      "Sum of all probabilities: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "green_balls = 12\n",
    "blue_balls = 8\n",
    "red_balls = 2\n",
    "yellow_balls = 1\n",
    "\n",
    "total_sum = probability(green_balls, total_balls) + probability(blue_balls,total_balls) + probability(red_balls,total_balls)+ probability(yellow_balls,total_balls) \n",
    "\n",
    "print(\"Sum of all probabilities:\", total_sum*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of an impossible outcome is zero. For example, it is impossible to roll a 7 with a standard six-sided die.\n",
    "\n",
    "- Probability of Impossible Outcome = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of a certain outcome is one. For example, it is certain that a value between 1 and 6 will occur when rolling a six-sided die.\n",
    "\n",
    "- Probability of Certain Outcome = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of taking 23 (all) ball(s) is: 100.0%\n",
      "Probability of certain outcome: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Probability of certain outcome:\", probability(total_balls, total_balls)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of an event not occurring, called the complement.\n",
    "\n",
    "This can be calculated by one minus the probability of the event, or 1 – P(A). For example, the probability of not rolling a 5 would be 1 – P(5) or 1 – 0.166 or about 0.833 or about 83.333%.\n",
    "\n",
    "- Probability of Not Event $A = 1 – P(A)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of taking 12 green ball(s) is: 52.17%\n",
      "Probability of Not Event: 47.83 %\n"
     ]
    }
   ],
   "source": [
    "# Lets take the probability of 12 green balls\n",
    "green_balls = 12\n",
    "draw_probability = probability(green_balls, total_balls)\n",
    "\n",
    "print(\"Probability of Not Event:\", str(round((1 - draw_probability)*100, 2)), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem of Conditional Probability\n",
    "Before diving into Bayes theorem, let’s review marginal, joint, and conditional probability.\n",
    "\n",
    "Recall that marginal probability is the probability of an event, irrespective of other random variables. If the random variable is independent, then it is the probability of the event directly, otherwise, if the variable is dependent upon other variables, then the marginal probability is the probability of the event summed over all outcomes for the dependent variables, called the sum rule.\n",
    "\n",
    "- **Marginal Probability**: The probability of an event irrespective of the outcomes of other random variables, e.g. $P(A)$.\n",
    "\n",
    "The joint probability is the probability of two (or more) simultaneous events, often described in terms of events A and B from two dependent random variables, e.g. X and Y. The joint probability is often summarized as just the outcomes, e.g. A and B.\n",
    "\n",
    "- **Joint Probability**: Probability of two (or more) simultaneous events, e.g. $P(A and B)$ or $P(A, B)$.\n",
    "\n",
    "The conditional probability is the probability of one event given the occurrence of another event, often described in terms of events A and B from two dependent random variables e.g. X and Y.\n",
    "\n",
    "- **Conditional Probability**: Probability of one (or more) event given the occurrence of another event, e.g. $P(A given B)$ or $P(A | B)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint probability distribution\n",
    "Now let's see what happens if we roll two dice. For each die, the outcomes are associated with a certain probability. We need two random variables to describe the game, let's say that $\\text{x}$ corresponds to the first die and $\\text{y}$ to the second one. We also have two probability mass functions associated with the random variables: $P(\\text{x})$ and $P(\\text{y})$. Here the possible values of the random variables (1, 2, 3, 4, 5 or 6) and the probability mass functions are actually the same for both dice, but it doesn't need to be the case.\n",
    "\n",
    "The joint probability distribution is useful in the cases where we are interested in the probability that $\\text{x}$ takes a specific value while $\\text{y}$ takes another specific value. For instance, what would be the probability to get a 1 with the first dice and 2 with the second dice? The probabilities corresponding to every pair of values are written $P(\\text{x}=x, \\text{y}=y)$ or $P(\\text{x}, \\text{y})$. This is what we call the joint probability.\n",
    "\n",
    "### Example 1.\n",
    "\n",
    "For example, let's calculate the probability to have a 1 with the first dice and a 2 in the second:\n",
    "\n",
    "$$\n",
    "P(\\text{x}=1, \\text{y}=2) = \\frac{1}{6} \\times \\frac{1}{6} = \\frac{1}{36} \\approx 0.028\n",
    "$$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional probability is the probability of one event given the occurrence of another event, often described in terms of events A and B from two dependent random variables e.g. $X$ and $Y$.\n",
    "\n",
    "- Conditional Probability: Probability of one (or more) event given the occurrence of another event, e.g. $P(A given B)$ or $P(A | B)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint probability can be calculated using the conditional probability; for example:\n",
    "\n",
    "- $P(A, B) = P(A | B) * P(B)$\n",
    "\n",
    "This is called the product rule. Importantly, the joint probability is symmetrical, meaning that:\n",
    "\n",
    "- $P(A, B) = P(B, A)$\n",
    "\n",
    "The conditional probability can be calculated using the joint probability; for example:\n",
    "\n",
    "- $P(A | B) = P(A, B) / P(B)$\n",
    "\n",
    "The conditional probability is not symmetrical; for example:\n",
    "\n",
    "- $P(A | B) != P(B | A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Naive Bayes algorithm?\n",
    "\n",
    "   It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "\n",
    "   For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.\n",
    "\n",
    "   Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n",
    "\n",
    "   Bayes theorem provides a way of calculating posterior probability $P(c|x)$ from $P(c)$, $P(x)$ and $P(x|c)$. Look at the equation below:\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{align}\n",
    "{\\large P(c|x) = \\frac{P(x|c)P(c)}{P(x)}}\n",
    "\\end{align}\n",
    "\n",
    "<br>\n",
    "\n",
    "## Naming the Terms in the Theorem\n",
    "The terms in the Bayes Theorem equation are given names depending on the context where the equation is used.\n",
    "\n",
    "Firstly, in general, the result $P(A|B)$ is referred to as the **posterior probability** and $P(A$) is referred to as the **prior probability**.\n",
    "\n",
    "- $P(A|B)$: Posterior probability.\n",
    "- $P(A)$: Prior probability.\n",
    "\n",
    "Sometimes $P(B|A)$ is referred to as the **likelihood** and $P(B)$ is referred to as the **evidence**.\n",
    "\n",
    "- $P(B|A)$: Likelihood.\n",
    "- $P(B)$: Evidence.\n",
    "\n",
    "This allows Bayes Theorem to be restated as:\n",
    "\n",
    "- **Posterior = Likelihood * Prior / Evidence**\n",
    "\n",
    "I can make this clear with a smoke and fire case.\n",
    "\n",
    "What is the probability that there is fire given that there is smoke?\n",
    "\n",
    "Where P(Fire) is the Prior, P(Smoke|Fire) is the Likelihood, and P(Smoke) is the evidence:\n",
    "\n",
    "- $P(Fire|Smoke)$ = $P(Smoke|Fire)$ * $P(Fire) / P(Smoke)$\n",
    "\n",
    "You can imagine the same situation with rain and clouds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostic Test Scenario\n",
    "An excellent and widely used example of the benefit of Bayes Theorem is in the analysis of a medical diagnostic test.\n",
    "\n",
    "Scenario: Consider a human population that may or may not have cancer (Cancer is True or False) and a medical test that returns positive or negative for detecting cancer (Test is Positive or Negative), e.g. like a mammogram for detecting breast cancer.\n",
    "\n",
    ">Problem: If a randomly selected patient has the test and it comes back positive, what is the probability that the patient has cancer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Calculation\n",
    "Medical diagnostic tests are not perfect; they have **error**.\n",
    "\n",
    "Sometimes a patient will have cancer, but the test will not detect it. This capability of the test to detect cancer is referred to as the sensitivity, or the **true positive rate**.\n",
    "\n",
    "In this case, I will contrive a sensitivity value for the test. The test is good, but not great, with a true positive rate or sensitivity of $85%$. That is, of all the people who have cancer and are tested, $85%$ of them will get a positive result from the test.\n",
    "\n",
    "- $P(Test=Positive | Cancer=True) = 0.85$\n",
    "\n",
    "Given this information, our intuition would suggest that there is an 85% probability that the patient has cancer.\n",
    "\n",
    "**Our intuitions of probability are wrong.**\n",
    "\n",
    "This type of error in interpreting probabilities is so common that it has its own name; it is referred to as the **base rate fallacy**.\n",
    "\n",
    "It has this name because the error in estimating the probability of an event is caused by ignoring the base rate. That is, it ignores the probability of a randomly selected person having cancer, regardless of the results of a diagnostic test.\n",
    "\n",
    "In this case, I can assume the probability of breast cancer is low, and use a contrived base rate value of one person in 5,000, or (0.0002) 0.02%.\n",
    "\n",
    "- $P(Cancer=True) = 0.02%$.\n",
    "\n",
    "I can correctly calculate the probability of a patient having cancer given a positive test result using Bayes Theorem.\n",
    "\n",
    "Let’s map our scenario onto the equation:\n",
    "\n",
    "- $P(A|B) = P(B|A) * P(A) / P(B)$\n",
    "\n",
    "- $P(Cancer=True | Test=Positive)$ = $P(Test=Positive|Cancer=True)$ * $P(Cancer=True)$ / $P(Test=Positive)$\n",
    "\n",
    "I know the probability of the test being positive given that the patient has cancer is 85%, and I know the base rate or the prior probability of a given patient having cancer is 0.02%; I can plug these values in:\n",
    "\n",
    "- $P(Cancer=True | Test=Positive)$ = 0.85 * 0.0002 / $P(Test=Positive)$\n",
    "\n",
    "I **don’t know** $P(Test=Positive)$, it’s not given directly.\n",
    "\n",
    "Instead, we can estimate it using:\n",
    "\n",
    "- $P(B)$ = $P(B|A)$ * $P(A)$ + $P(B|not A)$ * $P(not A)$\n",
    "\n",
    "- $P(Test=Positive)$ = $P(Test=Positive|Cancer=True)$ * $P(Cancer=True)$ + $P(Test=Positive|Cancer=False)$ * $P(Cancer=False)$\n",
    "\n",
    "Firstly, we can calculate $P(Cancer=False)$ as the complement of $P(Cancer=True)$, which we already know\n",
    "\n",
    "- $P(Cancer=False)$ = 1 – $P(Cancer=True)$\n",
    "- = 1 – 0.0002\n",
    "- = 0.9998\n",
    "\n",
    "Let’s plugin what we have:\n",
    "\n",
    "We can plug in our known values as follows:\n",
    "\n",
    "- $P(Test=Positive)$ = 0.85 * 0.0002 + $P(Test=Positive|Cancer=False)$ * 0.9998\n",
    "\n",
    "We still do not know the probability of a positive test result given no cancer.\n",
    "\n",
    "This requires additional information.\n",
    "\n",
    "Specifically, we need to know how good the test is at correctly identifying people that do not have cancer. That is, testing negative result $(Test=Negative)$ when the patient does not have cancer $(Cancer=False)$, called the **true negative rate** or the specificity.\n",
    "\n",
    "We will use a contrived specificity value of 95%.\n",
    "\n",
    "- $P(Test=Negative | Cancer=False)$ = 0.95\n",
    "\n",
    "With this final piece of information, we can calculate the **false positive** or **false alarm rate** as the complement of the **true negative rate**.\n",
    "\n",
    "- $P(Test=Positive|Cancer=False)$ = 1 – $P(Test=Negative | Cancer=False)$\n",
    "- = 1 – 0.95\n",
    "- = 0.05\n",
    "\n",
    "We can plug this false alarm rate into our calculation of $P(Test=Positive)$ as follows:\n",
    "\n",
    "- $P(Test=Positive)$ = 0.85 * 0.0002 + 0.05 * 0.9998\n",
    "- $P(Test=Positive)$ = 0.00017 + 0.04999\n",
    "- $P(Test=Positive)$ = 0.05016\n",
    "\n",
    "Excellent, so the probability of the test returning a positive result, regardless of whether the person has cancer or not is about **5%**.\n",
    "\n",
    "We now have enough information to calculate Bayes Theorem and estimate the probability of a randomly selected person having cancer if they get a positive test result.\n",
    "\n",
    "- $P(Cancer=True | Test=Positive)$ = $P(Test=Positive|Cancer=True)$ * $P(Cancer=True)$ / $P(Test=Positive)$\n",
    "- $P(Cancer=True | Test=Positive)$ = 0.85 * 0.0002 / 0.05016\n",
    "- $P(Cancer=True | Test=Positive)$ = 0.00017 / 0.05016\n",
    "- $P(Cancer=True | Test=Positive)$ = 0.003389154704944\n",
    "\n",
    "The calculation suggests that if the patient is informed they have cancer with this test, then there is only 0.33% chance that they have cancer.\n",
    "\n",
    "**It is a terrible diagnostic test!**\n",
    "\n",
    "The example also shows that the calculation of the conditional probability requires enough information.\n",
    "\n",
    "For example, if we have the values used in Bayes Theorem already, we can use them directly.\n",
    "\n",
    "This is rarely the case, and we typically have to calculate the bits we need and plug them in, as we did in this case. In our scenario we were given 3 pieces of information, the base rate, the  sensitivity (or true positive rate), and the specificity (or true negative rate).\n",
    "\n",
    "- Sensitivity: 85% of people with cancer will get a positive test result.\n",
    "- Base Rate: 0.02% of people have cancer.\n",
    "- Specificity: 95% of people without cancer will get a negative test result.\n",
    "\n",
    "I did not have the $P(Test=Positive)$, but we calculated it given what we already had available.\n",
    "\n",
    "I might imagine that Bayes Theorem allows us to be even more precise about a given scenario. For example, if we had more information about the patient (e.g. their age) and about the domain (e.g. cancer rates for age ranges), and in turn we could offer an even more accurate probability estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(A|B) = 0.339%\n"
     ]
    }
   ],
   "source": [
    "# calculate the probability of cancer patient and diagnostic test\n",
    " \n",
    "# calculate P(A|B) given P(A), P(B|A), P(B|not A)\n",
    "def bayes_theorem(p_a, p_b_given_a, p_b_given_not_a):\n",
    "    # calculate P(not A)\n",
    "    not_a = 1 - p_a\n",
    "    # calculate P(B)\n",
    "    p_b = p_b_given_a * p_a + p_b_given_not_a * not_a\n",
    "    # calculate P(A|B)\n",
    "    p_a_given_b = (p_b_given_a * p_a) / p_b\n",
    "    return p_a_given_b\n",
    " \n",
    "# P(A)\n",
    "p_a = 0.0002\n",
    "# P(B|A)\n",
    "p_b_given_a = 0.85\n",
    "# P(B|not A)\n",
    "p_b_given_not_a = 0.05\n",
    "# calculate P(A|B)\n",
    "result = bayes_theorem(p_a, p_b_given_a, p_b_given_not_a)\n",
    "# summarize\n",
    "print('P(A|B) = %.3f%%' % (result * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Naive Bayes algorithm work?\n",
    "Let’s understand it using an example. Below I have taken a training data set of weather and corresponding target variable ‘Play’ (suggesting possibilities of playing). Now, I need to classify whether players will play or not based on weather condition.\n",
    "\n",
    "- Step 1: Convert the data set into a frequency table\n",
    "\n",
    "- Step 2: Create Likelihood table by finding the probabilities like Overcast probability = 0.29 and probability of playing is 0.64.\n",
    "\n",
    "<img src=\"./images/NaiveBayesianFootball.webp\"/>\n",
    "\n",
    "- Step 3: Now, use Naive Bayesian equation to calculate the posterior probability for each class. The class with the highest posterior probability is the outcome of prediction.\n",
    "\n",
    "**Problem: Players will play if weather is sunny. Is this statement correct?**\n",
    "\n",
    "Now let's solve it.\n",
    "\n",
    "$P(Yes | Sunny)$ = $P( Sunny | Yes)$ * $P(Yes)$ / $P(Sunny)$\n",
    "\n",
    "Here I have $P(Sunny |Yes)$ = 3/9 = 0.33, $P(Sunny)$ = 5/14 = 0.36, $P(Yes)$= 9/14 = 0.64\n",
    "\n",
    "Now, $P(Yes | Sunny)$ = 0.33 * 0.64 / 0.36 = 0.60, which has higher probability.\n",
    "\n",
    "Naive Bayes uses a similar method to predict the probability of different class based on various attributes. This algorithm is mostly used in text classification and with problems having multiple classes.\n",
    "\n",
    "## Some of the applications of Naive Bayes Algorithms\n",
    "Real time Prediction: Naive Bayes is an eager learning classifier and it is sure fast. Thus, it could be used for making predictions in real time.\n",
    "- Multi class Prediction: This algorithm is also well known for multi class prediction feature. Here I can predict the probability of multiple classes of target variable.\n",
    "- Text classification/ Spam Filtering/ Sentiment Analysis: Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments)\n",
    "- Recommendation System: Naive Bayes Classifier and Collaborative Filtering together builds a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Flower Species Dataset\n",
    "The Iris Flower Dataset involves predicting the flower species given measurements of iris flowers.\n",
    "\n",
    "It is a multiclass classification problem. The number of observations for each class is balanced. There are 150 observations with 4 input variables and 1 output variable. The variable names are as follows:\n",
    "\n",
    "- Sepal length in cm.\n",
    "- Sepal width in cm.\n",
    "- Petal length in cm.\n",
    "- Petal width in cm.\n",
    "- Class\n",
    "\n",
    "A sample of the first 4 rows is listed below.\n",
    "```python\n",
    "4.9,3.0,1.4,0.2,Iris-setosa\n",
    "4.7,3.2,1.3,0.2,Iris-setosa\n",
    "4.6,3.1,1.5,0.2,Iris-setosa\n",
    "5.0,3.6,1.4,0.2,Iris-setosa\n",
    "...\n",
    "```\n",
    "\n",
    "<a href=\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.names\">More Information on Dataset (iris.names)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class\n",
       "0           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "1           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "2           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "3           5.0          3.6           1.4          0.2  Iris-setosa\n",
       "4           5.4          3.9           1.7          0.4  Iris-setosa"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Url from which I will download our iris.csv dataset\n",
    "iris = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(iris)\n",
    "\n",
    "# Read the file and asign the data in 'df'\n",
    "df = pd.read_csv(iris, sep=',')\n",
    "\n",
    "# Add column names to the DataFrame\n",
    "attributes = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n",
    "df.columns = attributes\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: orange;\">Step 1: Separate By Class</h1>\n",
    "\n",
    "I will need to calculate the probability of data by the class they belong to, the so-called base rate.\n",
    "\n",
    "This means that I will first need to separate my training data by class.\n",
    "\n",
    "I can create a dictionary object where each key is the class value and then add a list of all the records as the value in the dictionary.\n",
    "\n",
    "Below is a function named **separate_by_class()** that implements this approach. It assumes that the last column in each row is the class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[3.393533211, 2.331273381, 0]\n",
      "[3.110073483, 1.781539638, 0]\n",
      "[1.343808831, 3.368360954, 0]\n",
      "[3.582294042, 4.67917911, 0]\n",
      "[2.280362439, 2.866990263, 0]\n",
      "1\n",
      "[7.423436942, 4.696522875, 1]\n",
      "[5.745051997, 3.533989803, 1]\n",
      "[9.172168622, 2.511101045, 1]\n",
      "[7.792783481, 3.424088941, 1]\n",
      "[7.939820817, 0.791637231, 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class\n",
       "0           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "1           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "2           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "3           5.0          3.6           1.4          0.2  Iris-setosa\n",
       "4           5.4          3.9           1.7          0.4  Iris-setosa"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset by class values, returns a dictionary\n",
    "def separate_by_class(dataset):\n",
    "    separated = dict()\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        class_value = vector[-1]\n",
    "        if (class_value not in separated):\n",
    "            separated[class_value] = list()\n",
    "        separated[class_value].append(vector)\n",
    "    return separated\n",
    "\n",
    "# Test separating data by class\n",
    "dataset = [[3.393533211,2.331273381,0],\n",
    "    [3.110073483,1.781539638,0],\n",
    "    [1.343808831,3.368360954,0],\n",
    "    [3.582294042,4.67917911,0],\n",
    "    [2.280362439,2.866990263,0],\n",
    "    [7.423436942,4.696522875,1],\n",
    "    [5.745051997,3.533989803,1],\n",
    "    [9.172168622,2.511101045,1],\n",
    "    [7.792783481,3.424088941,1],\n",
    "    [7.939820817,0.791637231,1]]\n",
    "\n",
    "separated = separate_by_class(dataset)\n",
    "\n",
    "for label in separated:\n",
    "    print(label)\n",
    "    for row in separated[label]:\n",
    "        print(row)\n",
    "        \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: orange;\">Step 2: Summarize Dataset</h1>\n",
    "\n",
    "I need two statistics from a given set of data.\n",
    "\n",
    "I’ll see how these statistics are used in the calculation of probabilities in a few steps. The two statistics I require from a given dataset are the mean and the standard deviation (average deviation from the mean).\n",
    "\n",
    "The mean is the average value and can be calculated as:\n",
    "\n",
    "- mean = sum(x)/n * count(x)\n",
    "\n",
    "Where x is the list of values or a column I am looking.\n",
    "\n",
    "Below is a small function named mean() that calculates the mean of a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of a list of numbers\n",
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: #FF4500;\">In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range.</p>\n",
    "\n",
    "The sample standard deviation is calculated as the mean difference from the mean value. This can be calculated as:\n",
    "\n",
    "- standard deviation = sqrt((sum i to N (x_i – mean(x))^2) / N-1)\n",
    "\n",
    "You can see that I square the difference between the mean and a given value, calculate the average squared difference from the mean, then take the square root to return the units back to their original value.\n",
    "\n",
    "Below is a small function named standard_deviation() that calculates the standard deviation of a list of numbers. You will notice that it calculates the mean. It might be more efficient to calculate the mean of a list of numbers once and pass it to the standard_deviation() function as a parameter. You can explore this optimization if you’re interested later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of a list of numbers\n",
    "def stdev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
    "    return sqrt(variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I require the mean and standard deviation statistics to be calculated for each input attribute or each column of our data.\n",
    "\n",
    "I can do that by gathering all of the values for each column into a list and calculating the mean and standard deviation on that list. Once calculated, I can gather the statistics together into a list or tuple of statistics. Then, repeat this operation for each column in the dataset and return a list of tuples of statistics.\n",
    "\n",
    "Below is a function named **summarize_dataset()** that implements this approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean, stdev and count for each column in a dataset\n",
    "def summarize_dataset(dataset):\n",
    "    summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
    "    del(summaries[-1])\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I pass in the dataset to the zip() function with the * operator that separates the dataset (that is a list of lists) into separate lists for each row. The zip() function then iterates over each element of each row and returns a column from the dataset as a list of numbers. A clever little trick.\n",
    "\n",
    "I then calculate the mean, standard deviation and count of rows in each column. A tuple is created from these 3 numbers and a list of these tuples is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5.178333386499999, 2.7665845055177263, 10), (2.9984683241, 1.218556343617447, 10)]\n"
     ]
    }
   ],
   "source": [
    "# Summary on the contrived dataset that I showed earlier\n",
    "summary = summarize_dataset(dataset)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: orange\">Step 3: Summarize Data By Class</h1>\n",
    "\n",
    "I require statistics from our training dataset organized by class.\n",
    "\n",
    "Above, I have developed the **separate_by_class()** function to separate a dataset into rows by class. And we have developed summarize_dataset() function to calculate summary statistics for each column.\n",
    "\n",
    "I can put all of this together and summarize the columns in the dataset organized by class values.\n",
    "\n",
    "Below is a function named **summarize_by_class()** that implements this operation. The dataset is first split by class, then statistics are calculated on each subset. The results in the form of a list of tuples of statistics are then stored in a dictionary by their class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset by class then calculate statistics for each row\n",
    "def summarize_by_class(dataset):\n",
    "    separated = separate_by_class(dataset)\n",
    "    summaries = dict()\n",
    "    for class_value, rows in separated.items():\n",
    "        summaries[class_value] = summarize_dataset(rows)\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(2.7420144012, 0.9265683289298018, 5),\n",
       "  (3.0054686692, 1.1073295894898725, 5)],\n",
       " 1: [(7.6146523718, 1.2344321550313704, 5),\n",
       "  (2.9914679790000003, 1.4541931384601618, 5)]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_by_class(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: orange\">Step 4: Gaussian Probability Density Function</h1>\n",
    "\n",
    "Calculating the probability or likelihood of observing a given real-value like X1 is difficult.\n",
    "\n",
    "One way we can do this is to assume that X1 values are drawn from a distribution, such as a bell curve or Gaussian distribution.\n",
    "\n",
    "A Gaussian distribution can be summarized using only two numbers: the mean and the standard deviation. Therefore, with a little math, I can estimate the probability of a given value. This piece of math is called a Gaussian Probability Distribution Function (or Gaussian PDF) and can be calculated as:\n",
    "\n",
    "- f(x) = (1 / sqrt(2 * PI) * sigma) * exp(-((x-mean)^2 / (2 * sigma^2)))\n",
    "\n",
    "Where sigma is the standard deviation for x, mean is the mean for x and PI is the value of pi.\n",
    "\n",
    "Below is a function that implements this. I tried to split it up to make it more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Gaussian probability distribution function for x\n",
    "def calculate_probability(x, mean, stdev):\n",
    "    exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "    return (1 / (sqrt(2 * pi) * stdev)) * exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s test it out to see how it works. Below are some worked examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3989422804014327\n",
      "0.24197072451914337\n",
      "0.24197072451914337\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Gaussian probability distribution function for x\n",
    "def calculate_probability(x, mean, stdev):\n",
    "    exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "    return (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
    " \n",
    "# Test Gaussian PDF\n",
    "print(calculate_probability(1.0, 1.0, 1.0))\n",
    "print(calculate_probability(2.0, 1.0, 1.0))\n",
    "print(calculate_probability(0.0, 1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running it prints the probability of some input values. You can see that when the value is 1 and the mean and standard deviation is 1 our input is the most likely (top of the bell curve) and has the probability of 0.39.\n",
    "\n",
    "We can see that when we keep the statistics the same and change the x value to 1 standard deviation either side of the mean value (2 and 0 or the same distance either side of the bell curve) the probabilities of those input values are the same at 0.24."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: orange;\">Step 5: Class Probabilities</h1>\n",
    "\n",
    "Now it is time to use the statistics calculated from our training data to calculate probabilities for new data.\n",
    "\n",
    "Probabilities are calculated separately for each class. This means that we first calculate the probability that a new piece of data belongs to the first class, then calculate probabilities that it belongs to the second class, and so on for all the classes.\n",
    "\n",
    "The probability that a piece of data belongs to a class is calculated as follows:\n",
    "\n",
    "- $P(class|data)$ = $P(X|class)$ * $P(class)$\n",
    "\n",
    "You may note that this is different from the Bayes Theorem described above.\n",
    "\n",
    "The division has been removed to simplify the calculation.\n",
    "\n",
    "This means that the result is no longer strictly a probability of the data belonging to a class. The value is still maximized, meaning that the calculation for the class that results in the largest value is taken as the prediction. This is a common implementation simplification as we are often more interested in the class prediction rather than the probability.\n",
    "\n",
    "The input variables are treated separately, giving the technique it’s name “naive“. For the above example where we have 2 input variables, the calculation of the probability that a row belongs to the first class 0 can be calculated as:\n",
    "\n",
    "- $P(class=0|X1,X2)$ = $P(X1|class=0)$ * $P(X2|class=0)$ * $P(class=0)$\n",
    "\n",
    "Now you can see why we need to separate the data by class value. The Gaussian Probability Density function in the previous step is how we calculate the probability of a real value like X1 and the statistics we prepared are used in this calculation.\n",
    "\n",
    "Below is a function named **calculate_class_probabilities()** that ties all of this together.\n",
    "\n",
    "It takes a set of prepared summaries and a new row as input arguments.\n",
    "\n",
    "- First the total number of training records is calculated from the counts stored in the summary statistics. This is used in the calculation of the probability of a given class or P(class) as the ratio of rows with a given class of all rows in the training data.\n",
    "\n",
    "- Next, probabilities are calculated for each input value in the row using the Gaussian probability density function and the statistics for that column and of that class. Probabilities are multiplied together as they accumulated. This process is repeated for each class in the dataset.\n",
    "\n",
    "- Finally a dictionary of probabilities is returned with one entry for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probabilities of predicting each class for a given row\n",
    "def calculate_class_probabilities(summaries, row):\n",
    "    total_rows = sum([summaries[label][0][2] for label in summaries])\n",
    "    probabilities = dict()\n",
    "    for class_value, class_summaries in summaries.items():\n",
    "        probabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
    "        for i in range(len(class_summaries)):\n",
    "            mean, stdev, count = class_summaries[i]\n",
    "            probabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below first calculates the summary statistics by class for the training dataset, then uses these statistics to calculate the probability of the first record belonging to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.05032427673372076, 1: 0.00011557718379945765}\n"
     ]
    }
   ],
   "source": [
    "summaries = summarize_by_class(dataset)\n",
    "probabilities = calculate_class_probabilities(summaries, dataset[0])\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the probability of the first row belonging to the 0 class (0.0503) is higher than the probability of it belonging to the 1 class (0.0001). We would therefore correctly conclude that it belongs to the 0 class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
